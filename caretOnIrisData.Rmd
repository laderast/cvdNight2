---
title: "ML on the Iris Dataset"
author: "Ted Laderas"
date: "May 16, 2017"
output: html_document
---

In this Markdown document, I show how to run multiple machine learning methods on the iris dataset using the `caret` package. `caret` has a unified interface to over 140 modeling and predictive techniques, so it will be ideal for us to compare our methods. There is way more information about using `caret` here: https://www.r-project.org/nosvn/conferences/useR-2013/Tutorials/kuhn/user_caret_2up.pdf

Before you do anything else, make sure that you have the following packages installed:

```{r warning=FALSE, message=FALSE}
library(caret)
library(dplyr)
library(data.table)
library(dtplyr)
library(rpart)
library(party)
library(e1071)
```

Your number one step is to try using `knitr` to build this document and view the output. Use the `Knit HTML` button right above the source window. For the workshop, you will be modifying two different Markdown documents (`ldaAssignment.Rmd` and `CTreeAssignment.Rmd`) to share your work.

## Some Simple Data Transformation

The first step is to reduce the Iris dataset to a 2-class problem, to make it easier to understand. We'll use this dataset on the Classification Tree algorithm as well.

```{r message=FALSE}
#Here I set the random seed so I can reproduce all of the random subsetting
set.seed(11111)

#load the iris dataset
data(iris)

#select only "versicolor" and "virginica" species from data
#we need to recast Species as a factor to drop the "versicolor" level
iris2 <- iris %>% filter(Species %in% c("versicolor", "virginica")) %>% mutate(Species = factor(Species))

#confirm that we did the subsetting correct
summary(iris2)
```

## Building Test and Train Datasets

Here we subset the data by building a training set to train our learner, and holding out part of the  data to assess the performance on the dataset. Note that you won't have to do this on the cvd dataset, since we've already separated the two out for you. 

```{r}
#grab indices of the dataset that represent 85% of the data
trainingIndices <- createDataPartition(y = iris2$Species, p=.85,
                                       list=FALSE)

#show the first few training indices
trainingIndices[1:10]

#select the rows
trainData <- iris2[trainingIndices,]
#confirm the number of rows should be 80
nrow(trainData)

#build our test set using the R-indexing
#using the "-" operator
testData <- iris2[-trainingIndices,]

#confirm the number of rows (should be 20)
nrow(testData)
```

In summary: our training data has `r nrow(trainData)` samples and our training set has `r nrow(testData)` samples.

#Training Your Machine Learner

Here we'll train multiple machine learning algorithms using the `train` function.

Before you get started, you should have selected your cohort.

Think very carefully before you add a predictive feature/variable. Using all features will actually impact the interpretability of your model. You need to justify the inclusion of each variable. Here I've included only `Sepal.Width` and `Petal.Width` because you could argue that `Sepal.Length` and `Sepal.Width` might be providing the same information.

A full list of machine learning methods in `caret` is available here: http://topepo.github.io/caret/available-models.html Note that you will also have to install the corresponding packages listed for that method.

I'll show how to use three modeling methods here.  

  + `lda` - linear discriminant analysis, 
  + `rpart` - Classification and regression trees 

```{r}
#our straw man: logistic regression
logitIris <- train(Species ~ Sepal.Width + Petal.Width + Sepal.Length, method="glm", family="binomial",
                   data=trainData)

#train linear discriminant analysis method
ldaIris <- train(Species ~ Sepal.Width + Petal.Width, method= "lda", data=trainData)

#train classification and regression tree
cartIris <- train(Species ~ Sepal.Width + Petal.Width, method= "rpart", data=trainData)

```

## Assessing the models on the Test Set

Now that we have our models trained, we can evaluate them on our test dataset. To do this, we use the `predict` function, and pass both our trained learner `ldaIris` and our `testData` into `predict`.

Question: why don't we assess our models on the training data? What are the dangers of doing so?

```{r}
#Predict species on test data
classPredLDA <- predict(ldaIris, newdata=testData)

#Compare predictions directly with the truth
data.frame(classPredLDA, truth=testData$Species)

#calculate confusion Matrix and other measures of accuracy
confMatLDA <- confusionMatrix(testData$Species, classPredLDA)

#Show everything from `confusionMatrix`
confMatLDA

#access confusion matrix directly
confMatLDA$table

#Show accuracy values
confMatLDA$overall

#Show class agreement values
confMatLDA$byClass
```

## So which algorithm did best?

Let's run our predictions on the other learners as well, and compare accuracies:

```{r}
classPredCart <- predict(cartIris, newdata = testData)
classPredLogit <- predict(logitIris, newdata = testData)

#compare all the predictions directly
#were there any rows where the predictions didn't match?
data.frame(truth=testData$Species,logit=classPredLogit, LDA=classPredLDA, CART=classPredCart)
```

## Comparing Accuracies of our models

Here we compare the accuracies of our models.

```{r}
confMatCart <- confusionMatrix(classPredCart, testData$Species)
confMatLogit <- confusionMatrix(classPredLogit, testData$Species)

accuracyComparison = rbind(logit = confMatLogit$overall,
                       LDA = confMatLDA$overall,
                       CART = confMatCart$overall
                    )

accuracyComparison
```